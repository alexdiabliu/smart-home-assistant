{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccce626b",
   "metadata": {},
   "source": [
    "# Overview of Design\n",
    "Goal: Message + Intent --> Params\n",
    "\n",
    "Named Entity Recognition (NER) is a Natural Language Processing (NLP) task where a model learns to find and label certain parts of a sentence that represent 'entities', like names, dates, songs, places, artists, etc.\n",
    "In this case, NER will be used to identify where the entities are and what type they are in the provided user message\n",
    "This will work in conjunction with the intent determining model, which tells you what the user wants. This model will specify what they are talking about.\n",
    "\n",
    "#### Potential Avenues\n",
    "1. HuggingFace Transformers: May consider for future, but seems overkill for now with GPU training\n",
    "2. OpenAI/GPT: Needs Wi-Fi connection, unideal\n",
    "3. spaCy: Ideal for smaller datasets, this was the chosen model\n",
    "\n",
    "#### NER with spaCy Steps\n",
    "1. Tokenize the input (split into words)\n",
    "2. Embedding Layer Converts each token (word) into vector representation\n",
    "3. Feeds into a neural network (Convolutional / transition-based feature extractor --> Feedforward layers for tagging each token --> trained via backpropagation)\n",
    "4. Network outputs a label for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fcbda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f47df5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  input_sentence                         annotations\n",
      "0   Play something by Luis Fonsi  {'entities': [(18, 28, 'artist')]}\n",
      "1    I want to hear Shape of You    {'entities': [(15, 27, 'song')]}\n",
      "2  Put on some Bohemian Rhapsody    {'entities': [(12, 29, 'song')]}\n",
      "3                 Play Despacito     {'entities': [(5, 14, 'song')]}\n",
      "4            Start playing Queen  {'entities': [(14, 19, 'artist')]}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Annotated_Intent_Dataset.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76794783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('play something by luis fonsi', {'entities': [(18, 28, 'artist')]}),\n",
       " ('i want to hear shape of you', {'entities': [(15, 27, 'song')]}),\n",
       " ('put on some bohemian rhapsody', {'entities': [(12, 29, 'song')]}),\n",
       " ('play despacito', {'entities': [(5, 14, 'song')]}),\n",
       " ('start playing queen', {'entities': [(14, 19, 'artist')]})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "TRAIN_DATA = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row[\"input_sentence\"].lower()\n",
    "    # ast to evaluate a string that represents a python literal like a dict\n",
    "    annotations = ast.literal_eval(row[\"annotations\"].lower())\n",
    "    TRAIN_DATA.append((text, annotations))\n",
    "\n",
    "(TRAIN_DATA[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "417b7758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 239.2330\n",
      "Loss after epoch 2: 60.9828\n",
      "Loss after epoch 3: 19.0806\n",
      "Loss after epoch 4: 6.6742\n",
      "Loss after epoch 5: 22.4596\n",
      "Loss after epoch 6: 6.0620\n",
      "Loss after epoch 7: 0.0624\n",
      "Loss after epoch 8: 4.1384\n",
      "Loss after epoch 9: 32.0226\n",
      "Loss after epoch 10: 36.4206\n",
      "Loss after epoch 11: 4.5083\n",
      "Loss after epoch 12: 4.2346\n",
      "Loss after epoch 13: 0.0000\n",
      "Loss after epoch 14: 0.0000\n",
      "Loss after epoch 15: 0.0000\n",
      "Loss after epoch 16: 0.0000\n",
      "Loss after epoch 17: 0.0000\n",
      "Loss after epoch 18: 0.0000\n",
      "Loss after epoch 19: 0.0000\n",
      "Loss after epoch 20: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import spacy # nlp libary\n",
    "from spacy.training.example import Example\n",
    "from random import shuffle\n",
    "\n",
    "# Step 1: Create a blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Step 2: Add the Named Entity Recognizer (NER) component (this is what gets trained to find the params)\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Step 3: Add your custom labels to the NER component\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations[\"entities\"]:\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Step 4: Begin training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Step 5: Train the model\n",
    "for i in range(20):  # 20 epochs\n",
    "    shuffle(TRAIN_DATA)  # shuffle data each time\n",
    "    losses = {}\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], losses=losses)\n",
    "    print(f\"Loss after epoch {i+1}: {losses['ner']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18caa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "song hey jude\n",
      "artist the beatles\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "doc = nlp(\"Play into jude by wold beatles\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b9a05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: joblib/pkl can't be used here because a spaCy pipeline is much more complicated than the simple python objects joblib is designed for (numpy arrays, sklearn estimators, etc)\n",
    "\n",
    "nlp.to_disk(\"param_classifier\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my-env-name)",
   "language": "python",
   "name": "my-env-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
