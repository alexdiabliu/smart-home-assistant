{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccce626b",
   "metadata": {},
   "source": [
    "# Overview of Design\n",
    "Goal: Message + Intent --> Params\n",
    "\n",
    "Named Entity Recognition (NER) is a Natural Language Processing (NLP) task where a model learns to find and label certain parts of a sentence that represent 'entities', like names, dates, songs, places, artists, etc.\n",
    "In this case, NER will be used to identify where the entities are and what type they are in the provided user message\n",
    "This will work in conjunction with the intent determining model, which tells you what the user wants. This model will specify what they are talking about.\n",
    "\n",
    "#### Potential Avenues\n",
    "1. HuggingFace Transformers: May consider for future, but seems overkill for now with GPU training\n",
    "2. OpenAI/GPT: Needs Wi-Fi connection, unideal\n",
    "3. spaCy: Ideal for smaller datasets, this was the chosen model\n",
    "\n",
    "#### NER with spaCy Steps\n",
    "1. Tokenize the input (split into words)\n",
    "2. Embedding Layer Converts each token (word) into vector representation\n",
    "3. Feeds into a neural network (Convolutional / transition-based feature extractor --> Feedforward layers for tagging each token --> trained via backpropagation)\n",
    "4. Network outputs a label for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8fcbda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import os\n",
    "# os.chdir(\"src/intent_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f47df5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           input_sentence  \\\n",
      "0    Start The Beatles's Someone Like You   \n",
      "1                             Make it max   \n",
      "2  Alarm for 6:30 with note call with mom   \n",
      "3        Delete the morning workout alarm   \n",
      "4   Turn on the track Sweet Child O' Mine   \n",
      "\n",
      "                                         annotations  \n",
      "0  {\"entities\": [(20, 36, \"song\"), (6, 17, \"artis...  \n",
      "1        {\"entities\": [(8, 11, \"percent_of_total\")]}  \n",
      "2  {\"entities\": [(10, 14, \"time\"), (25, 38, \"labe...  \n",
      "3                  {\"entities\": [(11, 26, \"label\")]}  \n",
      "4                   {\"entities\": [(18, 37, \"song\")]}  \n"
     ]
    }
   ],
   "source": [
    "df_shuffled = pd.read_csv(\"NER_Intent_Dataset.csv\")\n",
    "# df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "print(df_shuffled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "76794783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"start the beatles's someone like you\",\n",
       "  {'entities': [(20, 36, 'song'), (6, 17, 'artist')]}),\n",
       " ('make it max', {'entities': [(8, 11, 'percent_of_total')]}),\n",
       " ('alarm for 6:30 with note call with mom',\n",
       "  {'entities': [(10, 14, 'time'), (25, 38, 'label')]}),\n",
       " ('delete the morning workout alarm', {'entities': [(11, 26, 'label')]}),\n",
       " (\"turn on the track sweet child o' mine\", {'entities': [(18, 37, 'song')]})]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "for _, row in df_shuffled.iterrows():\n",
    "    text = row[\"input_sentence\"].lower()\n",
    "    # ast to evaluate a string that represents a python literal like a dict\n",
    "    annotations = ast.literal_eval(row[\"annotations\"].lower())\n",
    "\n",
    "    TRAIN_DATA.append((text, annotations))\n",
    "    i+=1\n",
    "\n",
    "(TRAIN_DATA[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "417b7758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 503.2669\n",
      "Loss after epoch 2: 25.0873\n",
      "Loss after epoch 3: 12.2763\n",
      "Loss after epoch 4: 11.0867\n",
      "Loss after epoch 5: 0.6833\n",
      "Loss after epoch 6: 118.1117\n",
      "Loss after epoch 7: 48.2729\n",
      "Loss after epoch 8: 14.2991\n",
      "Loss after epoch 9: 14.9271\n",
      "Loss after epoch 10: 20.2093\n",
      "Loss after epoch 11: 8.7728\n",
      "Loss after epoch 12: 16.2256\n",
      "Loss after epoch 13: 50.6962\n",
      "Loss after epoch 14: 103.4880\n",
      "Loss after epoch 15: 15.7139\n",
      "Loss after epoch 16: 7.7269\n",
      "Loss after epoch 17: 5.0962\n",
      "Loss after epoch 18: 4.0778\n",
      "Loss after epoch 19: 31.7038\n",
      "Loss after epoch 20: 36.7922\n"
     ]
    }
   ],
   "source": [
    "import spacy # nlp libary\n",
    "from spacy.training.example import Example\n",
    "from random import shuffle\n",
    "\n",
    "# Step 1: Create a blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Step 2: Add the Named Entity Recognizer (NER) component (this is what gets trained to find the params)\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Step 3: Add your custom labels to the NER component\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations[\"entities\"]:\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Step 4: Begin training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Step 5: Train the model\n",
    "for i in range(20):  # 20 epochs\n",
    "    shuffle(TRAIN_DATA)  # shuffle data each time\n",
    "    losses = {}\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], losses=losses)\n",
    "\n",
    "    print(f\"Loss after epoch {i+1}: {losses['ner']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b18caa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 8am\n",
      "label breakfast\n",
      "-------------------\n",
      "song tomorrow at\n",
      "time 9am\n",
      "label running\n",
      "-------------------\n",
      "song godzilla\n",
      "artist eminem\n",
      "-------------------\n",
      "song godzilla\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "#test, should produce: song summertime sadness artist lana del ray\n",
    "def test(test_str):\n",
    "    output = nlp(test_str)\n",
    "    for ent in output.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "    print(\"-------------------\")\n",
    "\n",
    "test(\"set an alarm for 8am tomorrow for breakfast\")\n",
    "test(\"wake me up tomorrow at 9am for running\")\n",
    "test(\"play godzilla by eminem\")\n",
    "test(\"play godzilla\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0b9a05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: joblib/pkl can't be used here because a spaCy pipeline is much more complicated than the simple python objects joblib is designed for (numpy arrays, sklearn estimators, etc)\n",
    "\n",
    "nlp.to_disk(\"param_classifier\")  # Save\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my-env-name)",
   "language": "python",
   "name": "my-env-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
